{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66430b2b",
   "metadata": {},
   "source": [
    "# Neural Network Assignment: IMDB Sentiment Classification\n",
    "This notebook explores different neural network architectures to classify movie reviews from the IMDB dataset. Several variations of the model are tested, including changes in hidden layers, activation functions, loss functions, and regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30704c",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "The IMDB dataset is preprocessed by tokenizing and padding sequences to ensure uniform input length. This step is essential for feeding data into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e35020",
   "metadata": {},
   "source": [
    "## 2. Base Model\n",
    "The baseline model consists of two hidden layers with ReLU activation. The output layer uses a sigmoid activation function for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ed950",
   "metadata": {},
   "source": [
    "## 3. Model Variations\n",
    "- **Single Hidden Layer:** Tests performance with only one hidden layer.\n",
    "- **More Units per Layer:** Uses 32 neurons per layer instead of the default.\n",
    "- **Different Loss Function (MSE):** Instead of binary cross-entropy, mean squared error is used.\n",
    "- **Alternative Activation Function (Tanh):** Tests the impact of using `tanh` instead of `relu`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a43953",
   "metadata": {},
   "source": [
    "## 4. Regularization Techniques\n",
    "To prevent overfitting, dropout and L2 regularization will be applied to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d0195",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "Each model's accuracy and loss will be compared. Visualizations of the training history will be provided to analyze convergence and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biCbJWoMgdv-",
    "outputId": "5e9c9fca-5977-499a-93f0-4c3e9a813d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.4955 - loss: 377.2423 - val_accuracy: 0.4922 - val_loss: 35.9550\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4979 - loss: 24.5329 - val_accuracy: 0.4930 - val_loss: 8.8374\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4988 - loss: 7.1911 - val_accuracy: 0.4944 - val_loss: 4.5105\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4979 - loss: 3.8633 - val_accuracy: 0.4978 - val_loss: 3.2444\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4973 - loss: 3.0222 - val_accuracy: 0.4952 - val_loss: 2.1857\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5043 - loss: 1.9160 - val_accuracy: 0.5018 - val_loss: 1.8874\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5033 - loss: 1.6115 - val_accuracy: 0.5036 - val_loss: 1.6035\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4979 - loss: 1.4576 - val_accuracy: 0.5036 - val_loss: 1.4170\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5049 - loss: 1.1509 - val_accuracy: 0.4934 - val_loss: 1.3230\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5030 - loss: 1.1065 - val_accuracy: 0.4922 - val_loss: 1.1845\n"
     ]
    }
   ],
   "source": [
    "#1.Starting with Base Model (Two Hidden Layers)\n",
    "#Setting standard model using two hidden layers:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Loading IMDB dataset\n",
    "num_words = 10000  # to Keep top 10,000 words\n",
    "maxlen = 500  # Maximum review length\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "# Pad sequences\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Baseline Model: Two Hidden Layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(maxlen,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbyUQBLlkFqS",
    "outputId": "f17094e8-0b49-4846-ca01-0d53dc3e494f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with One Hidden Layer...\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5040 - loss: 302.7050 - val_accuracy: 0.4984 - val_loss: 58.9519\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4987 - loss: 38.0854 - val_accuracy: 0.5058 - val_loss: 8.1992\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5031 - loss: 6.4184 - val_accuracy: 0.5128 - val_loss: 3.6396\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5006 - loss: 3.2277 - val_accuracy: 0.5152 - val_loss: 2.3517\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5010 - loss: 2.2052 - val_accuracy: 0.5150 - val_loss: 1.7721\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5028 - loss: 1.5181 - val_accuracy: 0.5136 - val_loss: 1.4871\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5044 - loss: 1.2724 - val_accuracy: 0.5120 - val_loss: 1.2855\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5081 - loss: 1.1029 - val_accuracy: 0.5110 - val_loss: 1.1706\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5017 - loss: 0.9501 - val_accuracy: 0.5096 - val_loss: 1.0993\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4998 - loss: 0.8849 - val_accuracy: 0.5096 - val_loss: 1.0478\n",
      "\n",
      "Training Model with Three Hidden Layers...\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5034 - loss: 310.7643 - val_accuracy: 0.4928 - val_loss: 18.5915\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5057 - loss: 13.8687 - val_accuracy: 0.5028 - val_loss: 4.9087\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5050 - loss: 4.2704 - val_accuracy: 0.5016 - val_loss: 2.8079\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5118 - loss: 2.6907 - val_accuracy: 0.5106 - val_loss: 2.0306\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5090 - loss: 2.0935 - val_accuracy: 0.5058 - val_loss: 1.5833\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5119 - loss: 1.5818 - val_accuracy: 0.4924 - val_loss: 1.4123\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5044 - loss: 1.3594 - val_accuracy: 0.5064 - val_loss: 1.2428\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5127 - loss: 1.1757 - val_accuracy: 0.5114 - val_loss: 1.1469\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5069 - loss: 1.1168 - val_accuracy: 0.5082 - val_loss: 1.3305\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5057 - loss: 1.2979 - val_accuracy: 0.4948 - val_loss: 1.2395\n"
     ]
    }
   ],
   "source": [
    "# One Hidden Layer\n",
    "model_one_layer = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(maxlen,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_one_layer.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training Model with One Hidden Layer...\")\n",
    "history_one = model_one_layer.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n",
    "\n",
    "# Three Hidden Layers\n",
    "model_three_layers = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(maxlen,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_three_layers.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nTraining Model with Three Hidden Layers...\")\n",
    "history_three = model_three_layers.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ONAF-OwkQh-",
    "outputId": "92e9c325-1f7c-4c06-b80e-d1b6f3deeef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with 32 Hidden Units...\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4917 - loss: 261.5523 - val_accuracy: 0.5082 - val_loss: 76.8300\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5082 - loss: 60.2285 - val_accuracy: 0.4948 - val_loss: 35.7308\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5226 - loss: 26.0902 - val_accuracy: 0.4984 - val_loss: 16.5991\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5239 - loss: 12.2425 - val_accuracy: 0.4960 - val_loss: 10.2259\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5214 - loss: 7.1405 - val_accuracy: 0.5070 - val_loss: 5.9015\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5129 - loss: 4.4060 - val_accuracy: 0.5056 - val_loss: 4.3757\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5251 - loss: 3.0955 - val_accuracy: 0.5082 - val_loss: 3.8192\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5214 - loss: 2.6483 - val_accuracy: 0.5080 - val_loss: 2.8193\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5244 - loss: 1.9024 - val_accuracy: 0.5078 - val_loss: 2.6378\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5260 - loss: 1.6723 - val_accuracy: 0.4938 - val_loss: 2.2622\n",
      "\n",
      "Training Model with 64 Hidden Units...\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5046 - loss: 173.4286 - val_accuracy: 0.4952 - val_loss: 67.0094\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5264 - loss: 48.8404 - val_accuracy: 0.4950 - val_loss: 40.8427\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5576 - loss: 27.4971 - val_accuracy: 0.4940 - val_loss: 30.4325\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5768 - loss: 17.9656 - val_accuracy: 0.5008 - val_loss: 24.4170\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6005 - loss: 12.6478 - val_accuracy: 0.5012 - val_loss: 21.2368\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6219 - loss: 9.4675 - val_accuracy: 0.4892 - val_loss: 19.5059\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6454 - loss: 7.0837 - val_accuracy: 0.4974 - val_loss: 17.7267\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6691 - loss: 5.7486 - val_accuracy: 0.5040 - val_loss: 16.7499\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6900 - loss: 4.7121 - val_accuracy: 0.5040 - val_loss: 15.6187\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7102 - loss: 3.7964 - val_accuracy: 0.5008 - val_loss: 15.0794\n"
     ]
    }
   ],
   "source": [
    "# Model with 32 Units\n",
    "model_32_units = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(maxlen,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_32_units.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training Model with 32 Hidden Units...\")\n",
    "history_32 = model_32_units.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n",
    "\n",
    "# Model with 64 Units\n",
    "model_64_units = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(maxlen,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_64_units.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nTraining Model with 64 Hidden Units...\")\n",
    "history_64 = model_64_units.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH97ziNSkW8o",
    "outputId": "991b9b5a-1cfb-4eb7-d6bc-af6809e31ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with MSE Loss Function...\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.4957 - loss: 0.5040 - val_accuracy: 0.5020 - val_loss: 0.4973\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4935 - loss: 0.5058 - val_accuracy: 0.5008 - val_loss: 0.4990\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4956 - loss: 0.5035 - val_accuracy: 0.4998 - val_loss: 0.4997\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5073 - loss: 0.4920 - val_accuracy: 0.5064 - val_loss: 0.4927\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5065 - loss: 0.4930 - val_accuracy: 0.5056 - val_loss: 0.4944\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4975 - loss: 0.5023 - val_accuracy: 0.5022 - val_loss: 0.4976\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4932 - loss: 0.5068 - val_accuracy: 0.5036 - val_loss: 0.4963\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4972 - loss: 0.5027 - val_accuracy: 0.5020 - val_loss: 0.4979\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4983 - loss: 0.5014 - val_accuracy: 0.5032 - val_loss: 0.4960\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4998 - loss: 0.4994 - val_accuracy: 0.5122 - val_loss: 0.4869\n"
     ]
    }
   ],
   "source": [
    "#4. Model with MSE Loss Function\n",
    "model_mse = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(maxlen,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_mse.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training Model with MSE Loss Function...\")\n",
    "history_mse = model_mse.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQvj8OBKkcDn",
    "outputId": "b384da58-4cd0-4caa-d7b6-6b54f5f55876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with tanh Activation...\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5067 - loss: 0.7584 - val_accuracy: 0.4952 - val_loss: 0.7235\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5024 - loss: 0.7185 - val_accuracy: 0.5126 - val_loss: 0.7038\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5011 - loss: 0.7046 - val_accuracy: 0.5008 - val_loss: 0.6994\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4936 - loss: 0.6982 - val_accuracy: 0.4986 - val_loss: 0.6980\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5072 - loss: 0.6961 - val_accuracy: 0.5050 - val_loss: 0.6961\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5070 - loss: 0.6949 - val_accuracy: 0.5064 - val_loss: 0.6961\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5019 - loss: 0.6960 - val_accuracy: 0.5050 - val_loss: 0.6957\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5118 - loss: 0.6942 - val_accuracy: 0.4962 - val_loss: 0.6976\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5069 - loss: 0.6950 - val_accuracy: 0.5074 - val_loss: 0.6965\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5168 - loss: 0.6931 - val_accuracy: 0.5032 - val_loss: 0.6961\n"
     ]
    }
   ],
   "source": [
    "#5. Model using tanh activation\n",
    "model_tanh = keras.Sequential([\n",
    "    layers.Dense(16, activation='tanh', input_shape=(maxlen,)),\n",
    "    layers.Dense(16, activation='tanh'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tanh.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Model with tanh Activation...\")\n",
    "history_tanh = model_tanh.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsVA4ylGkhX9",
    "outputId": "1b652d85-c420-4fff-ab4f-4afff90fac88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with Regularization and Dropout...\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.4947 - loss: 381.0123 - val_accuracy: 0.4950 - val_loss: 17.8332\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4925 - loss: 26.7249 - val_accuracy: 0.4994 - val_loss: 4.7640\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4980 - loss: 7.0828 - val_accuracy: 0.5054 - val_loss: 3.0278\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4980 - loss: 4.4229 - val_accuracy: 0.5036 - val_loss: 2.3982\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4957 - loss: 3.4456 - val_accuracy: 0.5034 - val_loss: 2.0268\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5017 - loss: 2.7898 - val_accuracy: 0.5038 - val_loss: 1.8030\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4988 - loss: 2.6977 - val_accuracy: 0.5020 - val_loss: 1.6649\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5013 - loss: 1.6954 - val_accuracy: 0.5034 - val_loss: 1.5877\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5061 - loss: 1.6495 - val_accuracy: 0.5044 - val_loss: 1.5238\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4981 - loss: 1.5815 - val_accuracy: 0.5050 - val_loss: 1.4872\n"
     ]
    }
   ],
   "source": [
    "#6. Model with L2 Regularization and Dropout\n",
    "model_reg = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(maxlen,)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_reg.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Model with Regularization and Dropout...\")\n",
    "history_reg = model_reg.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
